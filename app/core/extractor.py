"""
Extrator principal do PNCP
"""

import os
import json
import time
import requests
import re
import asyncio
from datetime import datetime, timedelta, date
from supabase import create_client, Client
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

from .config import settings


class PNCPExtractor:
    """Extrator principal de editais do PNCP"""
    
    def __init__(self):
        # Supabase
        if not settings.is_configured():
            raise Exception("Configure SUPABASE_URL e SUPABASE_KEY no .env")
        
        self.supabase = create_client(settings.SUPABASE_URL, settings.SUPABASE_KEY)
        self.bucket_name = settings.STORAGE_BUCKET
        
        # Session para APIs
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Selenium
        self.driver = None
        
        # URLs
        self.url_base = f"{settings.PNCP_BASE_URL}/app/editais"
    
    def configurar_selenium(self):
        """Configura Selenium otimizado para Render"""
        chrome_options = Options()
        
        if settings.SELENIUM_HEADLESS:
            chrome_options.add_argument("--headless")
        
        # Configura√ß√µes espec√≠ficas para Render
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-images")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--allow-running-insecure-content")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        try:
            # Tenta usar webdriver-manager para Render
            from webdriver_manager.chrome import ChromeDriverManager
            from selenium.webdriver.chrome.service import Service
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            print("‚úÖ Selenium configurado com webdriver-manager")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro com webdriver-manager: {e}")
            try:
                # Fallback para configura√ß√£o manual
                self.driver = webdriver.Chrome(options=chrome_options)
                print("‚úÖ Selenium configurado manualmente")
                return True
            except Exception as e2:
                print(f"‚ùå Erro ao configurar Selenium: {e2}")
                return False
    
    def buscar_editais_recentes(self, data_filtro=None, max_paginas=10, limit_por_pagina=50):
        """Busca editais espec√≠ficos do dia no PNCP (estrat√©gia otimizada)"""
        if not data_filtro:
            data_filtro = (datetime.now() - timedelta(days=1)).date()
        
        print(f"üîç ESTRAT√âGIA OTIMIZADA: Buscando TODOS os editais de {data_filtro}")
        print(f"üìä Configura√ß√£o: at√© {max_paginas} p√°ginas √ó {limit_por_pagina} editais = m√°ximo {max_paginas * limit_por_pagina} editais")
        
        if not self.configurar_selenium():
            return []
        
        editais_encontrados = []
        
        try:
            # Busca com filtro de data espec√≠fico (se o PNCP suportar)
            data_formatada = data_filtro.strftime("%d/%m/%Y")
            
            for pagina in range(1, max_paginas + 1):
                # URL otimizada - busca por editais mais recentes primeiro
                url_pagina = f"{self.url_base}?q=&pagina={pagina}&tam_pagina={limit_por_pagina}&ordenacao=data_desc"
                
                print(f"   üìÑ P√°gina {pagina}: {url_pagina}")
                print(f"   üéØ Buscando editais de: {data_formatada}")
                
                self.driver.get(url_pagina)
                time.sleep(2)
                
                try:
                    time.sleep(1)
                    page_source = self.driver.page_source
                    soup = BeautifulSoup(page_source, 'html.parser')
                    containers = soup.find_all("a", class_="br-item")
                    
                    if not containers:
                        print(f"   ‚ùå Nenhum container encontrado na p√°gina {pagina}")
                        break
                    
                    print(f"   üìã Encontrados {len(containers)} editais na p√°gina {pagina}")
                    
                    editais_pagina = []
                    data_mais_antiga_pagina = None
                    
                    for container in containers:
                        try:
                            edital_data = self.extrair_dados_container(container)
                            if edital_data:
                                try:
                                    # Tenta diferentes formatos de data
                                    data_str = edital_data['ultima_atualizacao']
                                    data_edital = None
                                    
                                    if data_str:
                                        # Tenta formato DD/MM/YYYY
                                        try:
                                            data_edital = datetime.strptime(data_str, "%d/%m/%Y").date()
                                        except ValueError:
                                            # Tenta formato DD-MM-YYYY
                                            try:
                                                data_edital = datetime.strptime(data_str, "%d-%m-%Y").date()
                                            except ValueError:
                                                # Tenta formato YYYY-MM-DD
                                                try:
                                                    data_edital = datetime.strptime(data_str, "%Y-%m-%d").date()
                                                except ValueError:
                                                    print(f"     ‚ö†Ô∏è {edital_data['id_pncp']} - formato de data n√£o reconhecido: '{data_str}'")
                                    
                                    if data_edital:
                                        if not data_mais_antiga_pagina or data_edital < data_mais_antiga_pagina:
                                            data_mais_antiga_pagina = data_edital
                                        
                                        if data_edital >= data_filtro:
                                            editais_pagina.append(edital_data)
                                            print(f"     ‚úÖ {edital_data['id_pncp']} - {data_edital}")
                                        else:
                                            print(f"     ‚è≠Ô∏è {edital_data['id_pncp']} - data muito antiga: {data_edital}")
                                    else:
                                        # Se n√£o conseguiu converter a data, inclui mesmo assim
                                        editais_pagina.append(edital_data)
                                        print(f"     ‚ö†Ô∏è {edital_data['id_pncp']} - data inv√°lida: '{data_str}'")
                                
                                except Exception as e:
                                    print(f"     ‚ùå Erro ao processar data: {e}")
                                    editais_pagina.append(edital_data)
                        
                        except Exception as e:
                            print(f"     ‚ùå Erro ao processar container: {e}")
                            continue
                    
                    editais_encontrados.extend(editais_pagina)
                    print(f"   ‚úÖ {len(editais_pagina)} editais v√°lidos na p√°gina {pagina}")
                    
                    # Limite de seguran√ßa alto para pegar TODOS os editais
                    if len(editais_encontrados) >= 1000:  # Limite muito alto
                        print(f"   üõë Limite de seguran√ßa (1000 editais) atingido, parando busca")
                        break
                    
                    # Para apenas se TODA a p√°gina for anterior ao filtro
                    if data_mais_antiga_pagina and data_mais_antiga_pagina < data_filtro and len(editais_pagina) == 0:
                        print(f"   üõë P√°gina inteira anterior ao filtro ({data_mais_antiga_pagina}), parando")
                        break
                    
                    # Continua se ainda h√° editais v√°lidos na p√°gina
                    if len(editais_pagina) > 0:
                        print(f"   ‚úÖ P√°gina ainda tem editais v√°lidos, continuando...")
                    else:
                        print(f"   ‚ö†Ô∏è Nenhum edital v√°lido nesta p√°gina, mas continuando...")
                
                except Exception as e:
                    print(f"   ‚ùå Erro ao processar p√°gina {pagina}: {e}")
                    break
            
        finally:
            if self.driver:
                self.driver.quit()
        
        print(f"üéØ Total de editais encontrados: {len(editais_encontrados)}")
        return editais_encontrados
    
    def normalizar_data(self, data_str):
        """Normaliza diferentes formatos de data"""
        if not data_str:
            return ""
        
        # Remove espa√ßos extras
        data_str = data_str.strip()
        
        # Padr√µes comuns de data
        padroes = [
            r'(\d{1,2})/(\d{1,2})/(\d{4})',  # DD/MM/YYYY
            r'(\d{1,2})-(\d{1,2})-(\d{4})',  # DD-MM-YYYY
            r'(\d{4})-(\d{1,2})-(\d{1,2})',  # YYYY-MM-DD
        ]
        
        for padrao in padroes:
            match = re.search(padrao, data_str)
            if match:
                if len(match.groups()) == 3:
                    if len(match.group(1)) == 4:  # YYYY-MM-DD
                        return f"{match.group(3)}/{match.group(2)}/{match.group(1)}"
                    else:  # DD/MM/YYYY ou DD-MM-YYYY
                        return f"{match.group(1)}/{match.group(2)}/{match.group(3)}"
        
        return data_str
    
    def extrair_dados_container(self, container):
        """Extrai dados b√°sicos de um container"""
        try:
            link = container.get("href", "")
            id_pncp = ""
            if "/editais/" in link:
                id_pncp = link.split("/editais/")[-1]
            
            if not id_pncp:
                return None
            
            texto_completo = container.get_text(separator=" | ", strip=True)
            
            dados = {
                "link": f"{settings.PNCP_BASE_URL}{link}" if link.startswith("/") else link,
                "id_pncp": id_pncp,
                "texto_completo": texto_completo
            }
            
            # Extra√ß√£o com regex
            edital_match = re.search(r'Edital\s+n[¬∞¬∫]?\s*(\d+/\d+)', texto_completo, re.IGNORECASE)
            dados["edital"] = edital_match.group(0) if edital_match else ""
            
            modalidade_match = re.search(r'Modalidade[^:]*:\s*([^|]+)', texto_completo, re.IGNORECASE)
            dados["modalidade"] = modalidade_match.group(1).strip() if modalidade_match else ""
            
            # Regex mais robusto para data
            data_match = re.search(r'√öltima\s+Atualiza√ß√£o[^:]*:\s*([^|]+)', texto_completo, re.IGNORECASE)
            data_extraida = data_match.group(1).strip() if data_match else ""
            dados["ultima_atualizacao"] = self.normalizar_data(data_extraida)
            
            # Debug: mostra o que foi extra√≠do
            if not dados["ultima_atualizacao"]:
                print(f"     üîç Debug - Data extra√≠da: '{data_extraida}'")
                print(f"     üîç Debug - Texto completo: {texto_completo[:200]}...")
            
            orgao_match = re.search(r'√ìrg√£o[^:]*:\s*([^|]+)', texto_completo, re.IGNORECASE)
            dados["orgao"] = orgao_match.group(1).strip() if orgao_match else ""
            
            local_match = re.search(r'Local[^:]*:\s*([^|]+)', texto_completo, re.IGNORECASE)
            dados["local"] = local_match.group(1).strip() if local_match else ""
            
            objeto_match = re.search(r'Objeto[^:]*:\s*(.+)', texto_completo, re.IGNORECASE)
            dados["objeto"] = objeto_match.group(1).strip() if objeto_match else ""
            
            return dados
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro ao extrair dados: {e}")
            return None
    
    def extrair_edital_completo_hibrido(self, id_pncp, salvar_arquivos=True):
        """Extrai dados COMPLETOS: Selenium (p√°gina) + APIs (dados estruturados) + Arquivos"""
        try:
            if not id_pncp or "/" not in id_pncp:
                return None
            
            print(f"     üöÄ EXTRA√á√ÉO COMPLETA H√çBRIDA: {id_pncp}")
            
            # Separa componentes do ID
            cnpj, ano, numero = id_pncp.split('/')
            
            # === 1. EXTRA√á√ÉO VIA SELENIUM (P√ÅGINA DETALHADA) ===
            url_detalhada = f"{settings.PNCP_BASE_URL}/app/editais/{id_pncp}"
            print(f"     üìÑ Acessando p√°gina: {url_detalhada}")
            
            # Configura Selenium se necess√°rio
            if not self.driver:
                if not self.configurar_selenium():
                    return None
            
            # Acessa p√°gina
            self.driver.get(url_detalhada)
            time.sleep(3)  # Aguarda carregamento completo
            
            # Extrai HTML
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            texto_pagina = soup.get_text(separator=" ", strip=True)
            
            # === 2. EXTRA√á√ÉO VIA APIs (DADOS ESTRUTURADOS) ===
            base_url = f"{settings.PNCP_API_URL}/orgaos/{cnpj}/compras/{ano}/{numero}"
            print(f"     üîó Buscando APIs: {base_url}")
            
            # Busca dados das APIs
            itens = []
            historico = []
            arquivos = []
            dados_orgao = {}
            
            # API de Itens
            try:
                print(f"       üìã Buscando itens...")
                itens_response = self.session.get(f"{base_url}/itens", timeout=15)
                if itens_response.status_code == 200:
                    itens = itens_response.json()
                    print(f"       ‚úÖ {len(itens)} itens encontrados")
                else:
                    print(f"       ‚ö†Ô∏è Itens: Status {itens_response.status_code}")
            except Exception as e:
                print(f"       ‚ùå Erro itens: {e}")
            
            # API de Hist√≥rico
            try:
                print(f"       üìú Buscando hist√≥rico...")
                historico_response = self.session.get(f"{base_url}/historico", timeout=15)
                if historico_response.status_code == 200:
                    historico = historico_response.json()
                    print(f"       ‚úÖ {len(historico)} eventos no hist√≥rico")
                else:
                    print(f"       ‚ö†Ô∏è Hist√≥rico: Status {historico_response.status_code}")
            except Exception as e:
                print(f"       ‚ùå Erro hist√≥rico: {e}")
            
            # API de Arquivos
            try:
                print(f"       üìé Buscando arquivos...")
                arquivos_response = self.session.get(f"{base_url}/arquivos", timeout=15)
                if arquivos_response.status_code == 200:
                    arquivos = arquivos_response.json()
                    print(f"       ‚úÖ {len(arquivos)} arquivos encontrados")
                else:
                    print(f"       ‚ö†Ô∏è Arquivos: Status {arquivos_response.status_code}")
            except Exception as e:
                print(f"       ‚ùå Erro arquivos: {e}")
            
            # API do √ìrg√£o
            try:
                print(f"       üè¢ Buscando dados do √≥rg√£o...")
                orgao_response = self.session.get(f"{settings.PNCP_API_URL}/orgaos/{cnpj}", timeout=15)
                if orgao_response.status_code == 200:
                    dados_orgao = orgao_response.json()
                    print(f"       ‚úÖ Dados do √≥rg√£o obtidos")
                else:
                    print(f"       ‚ö†Ô∏è √ìrg√£o: Status {orgao_response.status_code}")
            except Exception as e:
                print(f"       ‚ùå Erro √≥rg√£o: {e}")
            
            # === 3. MONTA DADOS COMPLETOS ===
            dados = {
                "id_pncp": id_pncp,
                "link_licitacao": url_detalhada,
                "metodo_extracao": "hibrido_completo",
                "data_coleta": datetime.now().isoformat(),
                "cnpj_orgao": cnpj,
                "ano": int(ano),
                "numero": int(numero)
            }
            
            # === 4. EXTRA√á√ÉO DE DADOS DA P√ÅGINA (SELENIUM) ===
            print(f"     üìÑ Extraindo dados da p√°gina HTML...")
            
            # Data de divulga√ß√£o no PNCP
            data_divulgacao_match = re.search(r'Data\s+de\s+divulga√ß√£o\s+no\s+PNCP[:\s]*(\d{2}/\d{2}/\d{4})', texto_pagina, re.IGNORECASE)
            if not data_divulgacao_match:
                data_divulgacao_match = re.search(r'divulga√ß√£o\s+no\s+PNCP[:\s]*(\d{2}/\d{2}/\d{4})', texto_pagina, re.IGNORECASE)
            dados["data_divulgacao_pncp"] = data_divulgacao_match.group(1) if data_divulgacao_match else None
            
            # √öltima atualiza√ß√£o
            ultima_atualizacao_match = re.search(r'√öltima\s+atualiza√ß√£o[:\s]*(\d{2}/\d{2}/\d{4})', texto_pagina, re.IGNORECASE)
            dados["ultima_atualizacao"] = ultima_atualizacao_match.group(1) if ultima_atualizacao_match else datetime.now().strftime('%d/%m/%Y')
            
            # √ìrg√£o (prioriza dados da API, sen√£o extrai da p√°gina)
            if dados_orgao.get('razaoSocial'):
                dados["orgao"] = dados_orgao['razaoSocial']
            else:
                orgao_match = re.search(r'√ìrg√£o[:\s]*([^\n\r]{1,200})', texto_pagina, re.IGNORECASE)
                dados["orgao"] = orgao_match.group(1).strip() if orgao_match else ""
            
            # Local
            if dados_orgao.get('municipio') and dados_orgao.get('uf'):
                dados["local"] = f"{dados_orgao['municipio']}/{dados_orgao['uf']}"
            else:
                local_match = re.search(r'Local[:\s]*([^\n\r]{1,100})', texto_pagina, re.IGNORECASE)
                dados["local"] = local_match.group(1).strip() if local_match else ""
            
            # Modalidade (prioriza dados dos itens)
            if itens and itens[0].get('criterioJulgamentoNome'):
                dados["modalidade"] = itens[0]['criterioJulgamentoNome']
            else:
                modalidade_match = re.search(r'Modalidade\s+da\s+contrata√ß√£o[:\s]*([^\n\r]{1,100})', texto_pagina, re.IGNORECASE)
                dados["modalidade"] = modalidade_match.group(1).strip() if modalidade_match else ""
            
            # Dados espec√≠ficos da p√°gina
            amparo_match = re.search(r'Amparo\s+legal[:\s]*([^\n\r]{1,200})', texto_pagina, re.IGNORECASE)
            dados["amparo_legal"] = amparo_match.group(1).strip() if amparo_match else ""
            
            tipo_match = re.search(r'Tipo[:\s]*([^\n\r]{1,100})', texto_pagina, re.IGNORECASE)
            dados["tipo"] = tipo_match.group(1).strip() if tipo_match else ""
            
            modo_disputa_match = re.search(r'Modo\s+de\s+disputa[:\s]*([^\n\r]{1,100})', texto_pagina, re.IGNORECASE)
            dados["modo_disputa"] = modo_disputa_match.group(1).strip() if modo_disputa_match else ""
            
            # Datas
            data_inicio_match = re.search(r'Data\s+de\s+in√≠cio\s+de\s+recebimento\s+de\s+propostas[:\s]*(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2})', texto_pagina, re.IGNORECASE)
            dados["data_inicio_propostas"] = data_inicio_match.group(1) if data_inicio_match else ""
            
            data_fim_match = re.search(r'Data\s+fim\s+de\s+recebimento\s+de\s+propostas[:\s]*(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2})', texto_pagina, re.IGNORECASE)
            dados["data_fim_propostas"] = data_fim_match.group(1) if data_fim_match else ""
            
            # Data de abertura (m√∫ltiplos padr√µes)
            data_abertura_match = re.search(r'Data\s+de\s+abertura\s+das?\s+propostas[:\s]*(\d{2}/\d{2}/\d{4}(?:\s+\d{2}:\d{2})?)', texto_pagina, re.IGNORECASE)
            if not data_abertura_match:
                data_abertura_match = re.search(r'Data\s+da\s+sess√£o[:\s]*(\d{2}/\d{2}/\d{4}(?:\s+\d{2}:\d{2})?)', texto_pagina, re.IGNORECASE)
            dados["data_abertura"] = data_abertura_match.group(1) if data_abertura_match else None
            
            situacao_match = re.search(r'Situa√ß√£o[:\s]*([^\n\r]{1,100})', texto_pagina, re.IGNORECASE)
            dados["situacao"] = situacao_match.group(1).strip() if situacao_match else ""
            
            # Objeto (prioriza infer√™ncia dos itens)
            if itens:
                dados["objeto"] = self.inferir_objeto(itens)
            else:
                objeto_match = re.search(r'Objeto[:\s]*([^\n\r]{1,500})', texto_pagina, re.IGNORECASE)
                dados["objeto"] = objeto_match.group(1).strip() if objeto_match else ""
            
            registro_preco_match = re.search(r'Registro\s+de\s+pre√ßo[:\s]*([^\n\r]{1,50})', texto_pagina, re.IGNORECASE)
            dados["registro_preco"] = registro_preco_match.group(1).strip() if registro_preco_match else ""
            
            fonte_orcamentaria_match = re.search(r'Fonte\s+or√ßament√°ria[:\s]*([^\n\r]{1,200})', texto_pagina, re.IGNORECASE)
            dados["fonte_orcamentaria"] = fonte_orcamentaria_match.group(1).strip() if fonte_orcamentaria_match else ""
            
            unidade_compradora_match = re.search(r'Unidade\s+compradora[:\s]*([^\n\r]{1,200})', texto_pagina, re.IGNORECASE)
            dados["unidade_compradora"] = unidade_compradora_match.group(1).strip() if unidade_compradora_match else ""
            
            id_contratacao_match = re.search(r'Id\s+contrata√ß√£o\s+PNCP[:\s]*([^\n\r]{1,100})', texto_pagina, re.IGNORECASE)
            dados["id_contratacao_pncp"] = id_contratacao_match.group(1).strip() if id_contratacao_match else ""
            
            fonte_match = re.search(r'Fonte[:\s]*([^\n\r]{1,200})', texto_pagina, re.IGNORECASE)
            dados["fonte"] = fonte_match.group(1).strip() if fonte_match else ""
            
            # === 5. DADOS ESTRUTURADOS (APIs) ===
            print(f"     üìä Processando dados estruturados...")
            
            # Edital b√°sico
            dados["edital"] = f"Edital {numero}/{ano}"
            
            # Valor total
            valor_total = sum(item.get('valorTotal', 0) for item in itens)
            dados["valor_total_numerico"] = valor_total
            dados["valor"] = f"R$ {valor_total:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.') if valor_total > 0 else ""
            
            # Contadores
            dados["total_itens"] = len(itens)
            dados["total_anexos"] = len(arquivos)
            dados["total_historico"] = len(historico)
            
            # Dados processados
            dados["itens"] = itens
            dados["anexos"] = arquivos
            dados["historico"] = historico
            dados["itens_processados"] = len(itens) > 0
            dados["anexos_processados"] = len(arquivos) > 0
            dados["historico_processado"] = len(historico) > 0
            
            # Informa√ß√µes detalhadas
            dados["informacoes_detalhadas"] = {
                "metodo": "hibrido_completo",
                "data": datetime.now().isoformat(),
                "selenium": True,
                "apis_utilizadas": ["itens", "historico", "arquivos", "orgao"],
                "total_apis_sucesso": sum([
                    1 if itens else 0,
                    1 if historico else 0,
                    1 if arquivos else 0,
                    1 if dados_orgao else 0
                ])
            }
            
            # === 6. SALVAMENTO DE ARQUIVOS (SE SOLICITADO) ===
            if salvar_arquivos and arquivos:
                print(f"     üìé Processando {len(arquivos)} arquivos...")
                for i, arquivo in enumerate(arquivos, 1):
                    try:
                        print(f"       [{i}/{len(arquivos)}] {arquivo.get('nome', 'arquivo')}...")
                        arquivo_info = self.processar_arquivo(arquivo, id_pncp)
                        if arquivo_info:
                            arquivo.update(arquivo_info)
                            print(f"       ‚úÖ Arquivo processado e salvo no bucket")
                        else:
                            print(f"       ‚ö†Ô∏è Falha no processamento do arquivo")
                    except Exception as e:
                        print(f"       ‚ùå Erro no arquivo {i}: {e}")
            
            # === 7. RESUMO DOS DADOS EXTRA√çDOS ===
            print(f"     ‚úÖ EXTRA√á√ÉO COMPLETA FINALIZADA:")
            print(f"        üìÖ Data divulga√ß√£o PNCP: {dados.get('data_divulgacao_pncp', 'N/A')}")
            print(f"        üìÖ Data abertura: {dados.get('data_abertura', 'N/A')}")
            print(f"        üè¢ √ìrg√£o: {dados.get('orgao', 'N/A')[:50]}...")
            print(f"        üìã Modalidade: {dados.get('modalidade', 'N/A')}")
            print(f"        üìç Situa√ß√£o: {dados.get('situacao', 'N/A')}")
            print(f"        üí∞ Valor: {dados.get('valor', 'N/A')}")
            print(f"        üìä Itens: {dados.get('total_itens', 0)}")
            print(f"        üìé Anexos: {dados.get('total_anexos', 0)}")
            print(f"        üìú Hist√≥rico: {dados.get('total_historico', 0)}")
            
            return dados
            
        except Exception as e:
            print(f"     ‚ùå Erro ao extrair p√°gina detalhada: {e}")
            return None

    def extrair_edital_completo_api(self, id_pncp, salvar_arquivos=False):
        """Extrai dados completos via APIs do PNCP"""
        try:
            if not id_pncp or "/" not in id_pncp:
                return None
            
            cnpj, ano, numero = id_pncp.split('/')
            base_url = f"{settings.PNCP_API_URL}/orgaos/{cnpj}/compras/{ano}/{numero}"
            
            # Busca dados das APIs
            itens = []
            historico = []
            arquivos = []
            
            try:
                itens_response = self.session.get(f"{base_url}/itens", timeout=10)
                if itens_response.status_code == 200:
                    itens = itens_response.json()
            except:
                pass
            
            try:
                historico_response = self.session.get(f"{base_url}/historico", timeout=10)
                if historico_response.status_code == 200:
                    historico = historico_response.json()
            except:
                pass
            
            try:
                arquivos_response = self.session.get(f"{base_url}/arquivos", timeout=10)
                if arquivos_response.status_code == 200:
                    arquivos = arquivos_response.json()
            except:
                pass
            
            # Dados do √≥rg√£o
            dados_orgao = {}
            try:
                orgao_response = self.session.get(f"{settings.PNCP_API_URL}/orgaos/{cnpj}", timeout=10)
                if orgao_response.status_code == 200:
                    dados_orgao = orgao_response.json()
            except:
                pass
            
            # Monta dados para Supabase
            valor_total = sum(item.get('valorTotal', 0) for item in itens)
            valor_fmt = f"R$ {valor_total:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
            
            return {
                "id_pncp": id_pncp,
                "edital": f"Edital {numero}/{ano}",
                "modalidade": itens[0].get('criterioJulgamentoNome', '') if itens else '',
                "ultima_atualizacao": datetime.now().strftime('%d/%m/%Y'),
                "orgao": dados_orgao.get('razaoSocial', ''),
                "local": f"{dados_orgao.get('municipio', '')}/{dados_orgao.get('uf', '')}",
                "objeto": self.inferir_objeto(itens),
                "link_licitacao": f"{settings.PNCP_BASE_URL}/app/editais/{id_pncp}",
                "valor": valor_fmt if valor_total > 0 else "N√ÉO INFORMADO",
                "data_abertura": "",
                "situacao": itens[0].get('situacaoCompraItemNome', 'Em andamento') if itens else 'Em andamento',
                "valor_sigiloso": any(item.get('orcamentoSigiloso', False) for item in itens),
                "informacoes_detalhadas": {
                    "metodo": "final_otimizado",
                    "data": datetime.now().isoformat(),
                    "apis_utilizadas": ["itens", "historico", "arquivos", "orgao"]
                },
                "itens": [
                    {
                        "numero": item.get('numeroItem'),
                        "descricao": item.get('descricao', ''),
                        "quantidade": item.get('quantidade', 0),
                        "valor_unitario": item.get('valorUnitarioEstimado', 0),
                        "valor_total": item.get('valorTotal', 0)
                    } for item in itens
                ],
                "anexos": [
                    self.processar_arquivo_storage(arq, id_pncp, salvar_arquivos) for arq in arquivos if arq
                ],
                "historico": [
                    {
                        "data": evento.get('logManutencaoDataInclusao', ''),
                        "usuario": evento.get('usuarioNome', ''),
                        "tipo": evento.get('tipoLogManutencaoNome', '')
                    } for evento in historico
                ],
                "total_itens": len(itens),
                "total_anexos": len(arquivos),
                "total_historico": len(historico),
                "itens_processados": len(itens) > 0,
                "anexos_processados": len(arquivos) > 0,
                "historico_processado": len(historico) > 0,
                "valor_total_numerico": float(valor_total),
                "cnpj_orgao": cnpj,
                "ano": int(ano),
                "numero": int(numero),
                "metodo_extracao": "final_otimizado",
                "tempo_extracao": 0,
                "data_coleta": datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"‚ùå Erro ao extrair via API {id_pncp}: {e}")
            return None
    
    def inferir_objeto(self, itens):
        """Infere objeto baseado nos itens"""
        if not itens:
            return "N√£o informado"
        
        descricoes = [item.get('descricao', '').upper() for item in itens]
        primeira_desc = descricoes[0] if descricoes else "N√£o informado"
        return f"Aquisi√ß√£o/Contrata√ß√£o: {primeira_desc[:100]}"
    
    def processar_arquivo(self, arquivo, id_pncp):
        """Processa arquivo: download + upload para bucket"""
        try:
            nome_arquivo = arquivo.get('nome', 'arquivo_sem_nome')
            url_download = arquivo.get('url', '')
            tamanho = arquivo.get('tamanho', 0)
            
            print(f"         üìé Processando: {nome_arquivo}")
            print(f"         üîó URL: {url_download}")
            print(f"         üìä Tamanho: {tamanho} bytes")
            
            if not url_download:
                print(f"         ‚ùå URL de download n√£o encontrada")
                return {
                    "nome": nome_arquivo,
                    "tamanho": tamanho,
                    "upload_sucesso": False,
                    "erro": "URL n√£o encontrada",
                    "storage_url": None,
                    "data_upload": datetime.now().isoformat()
                }
            
            # === 1. DOWNLOAD DO ARQUIVO ===
            print(f"         ‚¨áÔ∏è Fazendo download...")
            
            try:
                response = self.session.get(url_download, timeout=30, stream=True)
                response.raise_for_status()
                
                # L√™ conte√∫do do arquivo
                conteudo_arquivo = response.content
                tamanho_real = len(conteudo_arquivo)
                
                print(f"         ‚úÖ Download conclu√≠do: {tamanho_real} bytes")
                
            except Exception as e:
                print(f"         ‚ùå Erro no download: {e}")
                return {
                    "nome": nome_arquivo,
                    "tamanho": tamanho,
                    "upload_sucesso": False,
                    "erro": f"Erro no download: {e}",
                    "storage_url": None,
                    "data_upload": datetime.now().isoformat()
                }
            
            # === 2. UPLOAD PARA BUCKET ===
            print(f"         ‚¨ÜÔ∏è Fazendo upload para bucket...")
            
            try:
                # Nome √∫nico para o arquivo no bucket
                import uuid
                nome_unico = f"{id_pncp.replace('/', '_')}_{uuid.uuid4().hex[:8]}_{nome_arquivo}"
                
                # Upload para Supabase Storage
                resultado_upload = self.supabase.storage.from_(self.bucket_name).upload(
                    file=conteudo_arquivo,
                    path=nome_unico,
                    file_options={
                        "content-type": self.detectar_content_type(nome_arquivo)
                    }
                )
                
                if resultado_upload:
                    # Gera URL p√∫blica
                    url_publica = self.supabase.storage.from_(self.bucket_name).get_public_url(nome_unico)
                    
                    print(f"         ‚úÖ Upload conclu√≠do!")
                    print(f"         üîó URL p√∫blica: {url_publica[:80]}...")
                    
                    return {
                        "nome": nome_arquivo,
                        "nome_bucket": nome_unico,
                        "tamanho": tamanho_real,
                        "upload_sucesso": True,
                        "storage_url": url_publica,
                        "bucket": self.bucket_name,
                        "data_upload": datetime.now().isoformat(),
                        "url_original": url_download
                    }
                else:
                    print(f"         ‚ùå Falha no upload")
                    return {
                        "nome": nome_arquivo,
                        "tamanho": tamanho_real,
                        "upload_sucesso": False,
                        "erro": "Falha no upload para bucket",
                        "storage_url": None,
                        "data_upload": datetime.now().isoformat()
                    }
                    
            except Exception as e:
                print(f"         ‚ùå Erro no upload: {e}")
                return {
                    "nome": nome_arquivo,
                    "tamanho": tamanho_real if 'tamanho_real' in locals() else tamanho,
                    "upload_sucesso": False,
                    "erro": f"Erro no upload: {e}",
                    "storage_url": None,
                    "data_upload": datetime.now().isoformat()
                }
                
        except Exception as e:
            print(f"         ‚ùå Erro geral no processamento: {e}")
            return {
                "nome": arquivo.get('nome', 'erro'),
                "tamanho": 0,
                "upload_sucesso": False,
                "erro": f"Erro geral: {e}",
                "storage_url": None,
                "data_upload": datetime.now().isoformat()
            }
    
    def detectar_content_type(self, nome_arquivo):
        """Detecta content-type baseado na extens√£o"""
        extensao = nome_arquivo.lower().split('.')[-1] if '.' in nome_arquivo else ''
        
        tipos = {
            'pdf': 'application/pdf',
            'doc': 'application/msword',
            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'xls': 'application/vnd.ms-excel',
            'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            'txt': 'text/plain',
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'png': 'image/png',
            'zip': 'application/zip',
            'rar': 'application/x-rar-compressed'
        }
        
        return tipos.get(extensao, 'application/octet-stream')
    
    def processar_arquivo_storage(self, arquivo_api, edital_id, fazer_upload=False):
        """Processa arquivo e opcionalmente faz upload para Storage"""
        try:
            url_original = arquivo_api.get('url')
            titulo = arquivo_api.get('titulo', 'arquivo')
            tipo = arquivo_api.get('tipo', 'PDF')
            
            arquivo_info = {
                "nome_original": titulo,
                "url_original": url_original,
                "tipo": tipo,
                "tamanho": 0,
                "storage_url": None,
                "upload_sucesso": False,
                "data_upload": datetime.now().isoformat()
            }
            
            if fazer_upload and url_original:
                try:
                    print(f"     üì• Baixando {titulo[:30]}...")
                    
                    response = self.session.get(url_original, timeout=30, stream=True)
                    response.raise_for_status()
                    
                    nome_limpo = re.sub(r'[<>:"/\\|?*]', '_', titulo)
                    nome_arquivo = f"{nome_limpo}.pdf"
                    
                    file_bytes = response.content
                    arquivo_info["tamanho"] = len(file_bytes)
                    arquivo_info["nome_arquivo"] = nome_arquivo
                    
                    print(f"     üìä Tamanho: {len(file_bytes):,} bytes")
                    
                    storage_path = f"editais/{edital_id}/{nome_arquivo}"
                    
                    print(f"     ‚òÅÔ∏è Enviando para Storage...")
                    
                    try:
                        result = self.supabase.storage.from_(self.bucket_name).upload(
                            path=storage_path,
                            file=file_bytes,
                            file_options={"content-type": "application/pdf"}
                        )
                        upload_ok = True
                    except Exception as upload_error:
                        if "duplicate" in str(upload_error).lower():
                            print(f"     üîÑ Arquivo existe, substituindo...")
                            try:
                                self.supabase.storage.from_(self.bucket_name).remove([storage_path])
                                result = self.supabase.storage.from_(self.bucket_name).upload(
                                    path=storage_path,
                                    file=file_bytes,
                                    file_options={"content-type": "application/pdf"}
                                )
                                upload_ok = True
                            except Exception as retry_error:
                                print(f"     ‚ùå Falha na substitui√ß√£o: {str(retry_error)[:50]}...")
                                upload_ok = False
                        else:
                            print(f"     ‚ùå Erro no upload: {str(upload_error)[:50]}...")
                            upload_ok = False
                    
                    if upload_ok:
                        storage_url = self.supabase.storage.from_(self.bucket_name).get_public_url(storage_path)
                        arquivo_info["storage_url"] = storage_url
                        arquivo_info["upload_sucesso"] = True
                        print(f"     ‚úÖ Upload conclu√≠do!")
                    else:
                        print(f"     ‚ùå Falha no upload")
                        
                except Exception as e:
                    print(f"     ‚ùå Erro no upload: {str(e)[:50]}...")
                    arquivo_info["erro_upload"] = str(e)
            
            return arquivo_info
            
        except Exception as e:
            print(f"     ‚ùå Erro ao processar arquivo: {e}")
            return None
    
    def salvar_supabase(self, dados):
        """Salva dados na tabela editais_completos"""
        try:
            print(f"   üîÑ Tentando salvar {dados['id_pncp']}...")
            
            try:
                result = self.supabase.table("editais_completos").insert(dados).execute()
                if result.data:
                    id_salvo = result.data[0].get("id")
                    print(f"   ‚úÖ Inserido com ID: {id_salvo}")
                    return id_salvo
                else:
                    print(f"   ‚ö†Ô∏è Insert sem dados retornados")
                    return None
            except Exception as insert_error:
                print(f"   ‚ö†Ô∏è Insert falhou: {str(insert_error)[:100]}...")
                
                if "duplicate" in str(insert_error).lower() or "unique" in str(insert_error).lower():
                    print(f"   üîÑ Tentando update...")
                    try:
                        result = self.supabase.table("editais_completos").update(dados).eq("id_pncp", dados["id_pncp"]).execute()
                        if result.data:
                            id_salvo = result.data[0].get("id")
                            print(f"   ‚úÖ Atualizado com ID: {id_salvo}")
                            return id_salvo
                        else:
                            print(f"   ‚ö†Ô∏è Update sem dados retornados")
                            return None
                    except Exception as update_error:
                        print(f"   ‚ùå Update falhou: {str(update_error)[:100]}...")
                        raise Exception(f"Insert e Update falharam: {insert_error}")
                else:
                    raise Exception(f"Erro no insert: {insert_error}")
            
        except Exception as e:
            print(f"   ‚ùå ERRO CR√çTICO ao salvar: {e}")
            
            if "policy" in str(e).lower() or "rls" in str(e).lower():
                print(f"   üí° PROBLEMA: Row Level Security bloqueando")
            elif "permission" in str(e).lower():
                print(f"   üí° PROBLEMA: Permiss√µes insuficientes")
            elif "network" in str(e).lower():
                print(f"   üí° PROBLEMA: Erro de rede")
            
            return None
    
    async def executar_extracao_dia(self, data_extracao=None, salvar_arquivos=False, max_editais=50):
        """Executa extra√ß√£o de um dia espec√≠fico com limites otimizados"""
        print("üöÄ INICIANDO EXTRA√á√ÉO DO DIA (OTIMIZADA)")
        print("=" * 50)
        
        start_time = time.time()
        
        if not data_extracao:
            data_extracao = (datetime.now() - timedelta(days=1)).date()
        elif isinstance(data_extracao, str):
            data_extracao = datetime.strptime(data_extracao, "%Y-%m-%d").date()
        
        print(f"üìÖ Data de extra√ß√£o: {data_extracao}")
        print(f"üìä Limite m√°ximo de editais: {max_editais}")
        print(f"üíæ Salvar arquivos: {salvar_arquivos}")
        
        # 1. Busca TODOS os editais do dia anterior (sem limites)
        editais_encontrados = self.buscar_editais_recentes(
            data_filtro=data_extracao,
            max_paginas=20,  # Aumentado para garantir cobertura TOTAL
            limit_por_pagina=100  # M√°ximo por p√°gina para efici√™ncia
        )
        
        if not editais_encontrados:
            return {
                "success": False,
                "message": f"Nenhum edital encontrado para {data_extracao}",
                "total_encontrados": 0,
                "total_salvos": 0,
                "tempo_execucao": round(time.time() - start_time, 2)
            }
        
        # 2. PROCESSA TODOS OS EDITAIS (sem limite)
        print(f"üéØ Processando TODOS os {len(editais_encontrados)} editais do dia anterior")
        
        # 3. Extrai e salva cada edital
        print(f"\nüìã Processando {len(editais_encontrados)} editais...")
        
        salvos = []
        erros = []
        
        for i, edital_basico in enumerate(editais_encontrados, 1):
            try:
                id_pncp = edital_basico.get("id_pncp")
                if not id_pncp:
                    continue
                
                print(f"[{i}/{len(editais_encontrados)}] üìã {id_pncp}")
                
                # Verifica se j√° existe (SEMPRE ATUALIZA)
                existing = self.supabase.table("editais_completos")\
                    .select("id, ultima_atualizacao")\
                    .eq("id_pncp", id_pncp)\
                    .execute()
                
                edital_existente = None
                if existing.data:
                    edital_existente = existing.data[0]
                    print(f"   üîÑ J√° existe (ID: {edital_existente['id']}) - EXTRAINDO PARA ATUALIZAR...")
                else:
                    print(f"   ‚ú® Novo edital - EXTRAINDO PARA INSERIR...")
                
                # Extrai dados completos: Selenium + APIs + Arquivos
                dados_completos = self.extrair_edital_completo_hibrido(id_pncp, salvar_arquivos=salvar_arquivos)
                
                if dados_completos:
                    # Salva (INSERT ou UPDATE autom√°tico)
                    supabase_id = self.salvar_supabase(dados_completos)
                    if supabase_id:
                        salvos.append(id_pncp)
                        if edital_existente:
                            print(f"   ‚úÖ ATUALIZADO ID: {supabase_id}")
                        else:
                            print(f"   ‚úÖ INSERIDO ID: {supabase_id}")
                    else:
                        print("   ‚ùå FALHA AO SALVAR NO SUPABASE")
                        erros.append({"id_pncp": id_pncp, "erro": "Falha ao salvar no Supabase"})
                else:
                    print("   ‚ùå Falha na extra√ß√£o de dados completos")
                    erros.append({"id_pncp": id_pncp, "erro": "Falha na extra√ß√£o de dados"})
                
                # Pausa reduzida
                await asyncio.sleep(0.2)  # Reduzido de 0.3 para 0.2
                
            except Exception as e:
                print(f"   ‚ùå Erro: {e}")
                erros.append({"id_pncp": id_pncp, "erro": str(e)})
        
        tempo_total = round(time.time() - start_time, 2)
        
        resultado = {
            "success": True,
            "message": f"Extra√ß√£o otimizada conclu√≠da em {tempo_total}s",
            "data_extracao": str(data_extracao),
            "total_encontrados": len(editais_encontrados),
            "total_salvos": len(salvos),
            "total_erros": len(erros),
            "tempo_execucao": tempo_total,
            "editais_salvos": salvos,
            "erros": erros,
            "configuracao": {
                "max_editais": max_editais,
                "salvar_arquivos": salvar_arquivos,
                "max_paginas": 2,
                "limit_por_pagina": 25
            }
        }
        
        print(f"\nüéâ EXTRA√á√ÉO OTIMIZADA CONCLU√çDA!")
        print(f"üìä Encontrados: {resultado['total_encontrados']}")
        print(f"üíæ Salvos: {resultado['total_salvos']}")
        print(f"‚ùå Erros: {resultado['total_erros']}")
        print(f"‚è±Ô∏è Tempo: {tempo_total}s")
        
        return resultado
    
    async def executar_extracao_inteligente(self, data_extracao=None, salvar_arquivos=False, max_editais=50):
        """Executa extra√ß√£o inteligente: sempre D-1, verifica existentes e atualiza mudan√ßas"""
        print("üß† INICIANDO EXTRA√á√ÉO INTELIGENTE")
        print("=" * 50)
        
        start_time = time.time()
        
        if not data_extracao:
            data_extracao = (datetime.now() - timedelta(days=1)).date()
        elif isinstance(data_extracao, str):
            data_extracao = datetime.strptime(data_extracao, "%Y-%m-%d").date()
        
        print(f"üìÖ Data de extra√ß√£o: {data_extracao}")
        print(f"üìä Limite m√°ximo de editais: {max_editais}")
        print(f"üíæ Salvar arquivos: {salvar_arquivos}")
        
        # 1. Busca editais do dia anterior
        editais_encontrados = self.buscar_editais_recentes(
            data_filtro=data_extracao,
            max_paginas=3,  # Busca mais p√°ginas para garantir cobertura
            limit_por_pagina=25
        )
        
        if not editais_encontrados:
            return {
                "success": False,
                "message": f"Nenhum edital encontrado para {data_extracao}",
                "total_encontrados": 0,
                "total_novos": 0,
                "total_atualizados": 0,
                "total_erros": 0,
                "tempo_execucao": round(time.time() - start_time, 2)
            }
        
        # 2. Limita o n√∫mero de editais processados
        # PROCESSA TODOS OS EDITAIS (estrat√©gia inteligente)
        print(f"üéØ Processando TODOS os {len(editais_encontrados)} editais com verifica√ß√£o inteligente")
        
        # 3. Processa cada edital com verifica√ß√£o inteligente
        print(f"\nüìã Processando {len(editais_encontrados)} editais...")
        
        novos = []
        atualizados = []
        erros = []
        
        for i, edital_basico in enumerate(editais_encontrados, 1):
            try:
                id_pncp = edital_basico.get("id_pncp")
                if not id_pncp:
                    continue
                
                print(f"[{i}/{len(editais_encontrados)}] üìã {id_pncp}")
                
                # Verifica se j√° existe na base
                existing = self.supabase.table("editais_completos")\
                    .select("id, ultima_atualizacao, updated_at")\
                    .eq("id_pncp", id_pncp)\
                    .execute()
                
                if existing.data:
                    edital_existente = existing.data[0]
                    print(f"   üîÑ J√° existe (ID: {edital_existente['id']})")
                    
                    # Verifica se precisa atualizar (comparando datas)
                    data_existente = edital_existente.get('ultima_atualizacao', '')
                    data_nova = edital_basico.get('ultima_atualizacao', '')
                    
                    if data_nova != data_existente:
                        print(f"   üìÖ Atualiza√ß√£o detectada: {data_existente} ‚Üí {data_nova}")
                        
                        # Extrai dados completos atualizados: Selenium + APIs + Arquivos
                        dados_completos = self.extrair_edital_completo_hibrido(id_pncp, salvar_arquivos=salvar_arquivos)
                        
                        if dados_completos:
                            # Atualiza registro existente
                            try:
                                result = self.supabase.table("editais_completos")\
                                    .update(dados_completos)\
                                    .eq("id_pncp", id_pncp)\
                                    .execute()
                                
                                if result.data:
                                    atualizados.append(id_pncp)
                                    print(f"   ‚úÖ ATUALIZADO ID: {edital_existente['id']}")
                                else:
                                    print("   ‚ùå FALHA AO ATUALIZAR")
                                    erros.append({"id_pncp": id_pncp, "erro": "Falha ao atualizar"})
                            except Exception as e:
                                print(f"   ‚ùå Erro na atualiza√ß√£o: {e}")
                                erros.append({"id_pncp": id_pncp, "erro": str(e)})
                        else:
                            print("   ‚ùå Falha na extra√ß√£o de dados atualizados")
                            erros.append({"id_pncp": id_pncp, "erro": "Falha na extra√ß√£o de dados"})
                    else:
                        print(f"   ‚è≠Ô∏è Sem mudan√ßas, mantendo existente")
                        continue
                else:
                    print(f"   üÜï Novo edital detectado")
                    
                    # Extrai dados completos: Selenium + APIs + Arquivos para novo edital
                    dados_completos = self.extrair_edital_completo_hibrido(id_pncp, salvar_arquivos=salvar_arquivos)
                    
                    if dados_completos:
                        # Salva novo registro
                        supabase_id = self.salvar_supabase(dados_completos)
                        if supabase_id:
                            novos.append(id_pncp)
                            print(f"   ‚úÖ NOVO SALVO ID: {supabase_id}")
                        else:
                            print("   ‚ùå FALHA AO SALVAR NOVO")
                            erros.append({"id_pncp": id_pncp, "erro": "Falha ao salvar novo"})
                    else:
                        print("   ‚ùå Falha na extra√ß√£o de dados para novo")
                        erros.append({"id_pncp": id_pncp, "erro": "Falha na extra√ß√£o de dados"})
                
                # Pausa reduzida
                await asyncio.sleep(0.2)
                
            except Exception as e:
                print(f"   ‚ùå Erro: {e}")
                erros.append({"id_pncp": id_pncp, "erro": str(e)})
        
        tempo_total = round(time.time() - start_time, 2)
        
        resultado = {
            "success": True,
            "message": f"Extra√ß√£o inteligente conclu√≠da em {tempo_total}s",
            "data_extracao": str(data_extracao),
            "total_encontrados": len(editais_encontrados),
            "total_novos": len(novos),
            "total_atualizados": len(atualizados),
            "total_erros": len(erros),
            "tempo_execucao": tempo_total,
            "editais_novos": novos,
            "editais_atualizados": atualizados,
            "erros": erros,
            "configuracao": {
                "max_editais": max_editais,
                "salvar_arquivos": salvar_arquivos,
                "max_paginas": 3,
                "limit_por_pagina": 25,
                "estrategia": "inteligente"
            }
        }
        
        print(f"\nüéâ EXTRA√á√ÉO INTELIGENTE CONCLU√çDA!")
        print(f"üìä Encontrados: {resultado['total_encontrados']}")
        print(f"üÜï Novos: {resultado['total_novos']}")
        print(f"üîÑ Atualizados: {resultado['total_atualizados']}")
        print(f"‚ùå Erros: {resultado['total_erros']}")
        print(f"‚è±Ô∏è Tempo: {tempo_total}s")
        
        return resultado